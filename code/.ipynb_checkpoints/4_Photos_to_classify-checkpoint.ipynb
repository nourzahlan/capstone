{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Imports](#Imports-and-Configuration)\n",
    "- [Photo Downloads](#Photo-Downloads)\n",
    "- [Save Results](#Save-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages and configure APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import geocoder\n",
    "import xml\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import flickr_api as flickr\n",
    "import googlemaps\n",
    "import time\n",
    "import requests\n",
    "from googlemaps import convert\n",
    "flickr.set_keys(api_key = '4ea697590240af7b4ced24c4e46b3d41', api_secret = '855a29a3e8cd2677')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../api_key_maps.txt') as f:\n",
    "    api_key = f.readline()\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.flickr.com/services/oauth/authorize?oauth_token=72157708599786124-9e19adedc0ba741b&perms=write\n"
     ]
    }
   ],
   "source": [
    "# source https://github.com/alexis-mignon/python-flickr-api/wiki/Flickr-API-Keys-and-Authentication\n",
    "a = flickr.auth.AuthHandler() # creates a new AuthHandler object\n",
    "perms = \"write\" # set the required permissions\n",
    "url = a.get_authorization_url(perms)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.set_verifier('1141080de8028213') # copy your oauth_verifier tag here!\n",
    "flickr.set_auth_handler(a)\n",
    "a.save('.auth.txt')\n",
    "flickr.set_auth_handler(\".auth.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nashville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Memphis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dallas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cities\n",
       "0   New York\n",
       "1  Nashville\n",
       "2    Memphis\n",
       "3     Dallas"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the route df with the stopover city names\n",
    "route_df = pd.read_csv('../datasets/route.csv')\n",
    "route_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "route_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with city names\n",
    "cities = []\n",
    "for i in range(route_df.shape[0]):\n",
    "    cities.append(route_df.iloc[i]['Cities'])\n",
    "\n",
    "# List with city center coordinates\n",
    "def get_latlong(address):\n",
    "    g = geocoder.arcgis(address)\n",
    "    return tuple(g.latlng)\n",
    "\n",
    "# get the coordinates of each city\n",
    "cities_coord = [get_latlong(city) for city in cities]\n",
    "\n",
    "# Dict with bounding boxes\n",
    "# This will be used to make sure we are out of the cities once we start looking for images along the road\n",
    "bounding_box = {a: {} for a in cities}\n",
    "for index, city in enumerate(cities):\n",
    "    g = geocoder.arcgis(cities[index])\n",
    "    bounding_box[city]['max_lat'] = g.bbox['northeast'][0]\n",
    "    bounding_box[city]['min_lat'] = g.bbox['southwest'][0]\n",
    "    bounding_box[city]['max_lng'] = g.bbox['northeast'][1]\n",
    "    bounding_box[city]['min_lng'] = g.bbox['southwest'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stopover cities, dowload photos with accuracy of 11 which means photos around the city.\n",
    "Along the route, download photos within a 10 km radius.\n",
    "So : \n",
    "- get route from 1 city to another\n",
    "- search radius of 10 km around every step end locations\n",
    "- stop when reaching stopover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading photos in  New York\n",
      "downloading photos in  Nashville\n",
      "downloading photos in  Memphis\n",
      "downloading photos in  Dallas\n"
     ]
    }
   ],
   "source": [
    "# download photos for every city\n",
    "# initialize dataframe with columns for each feature we want to save from our images\n",
    "file_number = 1\n",
    "photo_info_df = pd.DataFrame(columns = ['filename', 'city', 'url', 'latitude', 'longitude'])\n",
    "\n",
    "# Download photos\n",
    "for i in range(len(cities)):\n",
    "    print('downloading photos in ', cities[i])\n",
    "    \n",
    "    # initializing page number and urls for each city. urls list helps us keep track of the number of images we have\n",
    "    page_number = 1 \n",
    "    urls = []\n",
    "    # loop through cities and get 3000 images for each\n",
    "    # if the API takes too long and cannot fetch enough images we exit the loop\n",
    "    # by setting max page number to 50, this would mean the API couldnt get images by the 50th page\n",
    "    while ((len(urls) < 3000) and (page_number < 50)):\n",
    "        # in this case we also check the views to get somewhat popular images \n",
    "        photo_search = flickr.Photo.search(api_key = '4ea697590240af7b4ced24c4e46b3d41',  \n",
    "                                           per_page = 500,\n",
    "                                           lat = cities_coord[i][0],\n",
    "                                           lon = cities_coord[i][1],\n",
    "                                           page = page_number,\n",
    "                                           accuracy = 11,\n",
    "                                           content_type = 1,\n",
    "                                           min_taken_date = 2016,\n",
    "                                           media = 'photos', \n",
    "                                           extras = ['views']) \n",
    "        for photo in photo_search:\n",
    "            # only add pictures that have at least 20 views \n",
    "            if (photo.views >= 20):\n",
    "                try:\n",
    "                    url = photo.get('url_c')\n",
    "                    urls.append(url)\n",
    "                    path = '../photos_to_classify/' + str(file_number) + '.jpg'\n",
    "                    urllib.request.urlretrieve(url, path)\n",
    "                    image = Image.open(path)\n",
    "                    image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "                    image.save(path)\n",
    "                    # save information relevant to future use in a dataframe\n",
    "                    photo_info_df.loc[file_number-1] = [path, cities[i], url,\n",
    "                                                        photo.getLocation().latitude, \n",
    "                                                        photo.getLocation().longitude]\n",
    "                    file_number += 1\n",
    "                except:\n",
    "                    pass                       \n",
    "        page_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12052, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save how many photos were downloaded for each city\n",
    "photo_info_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading pictures in the  New York to Nashville route\n",
      "downloading pictures in the  Nashville to Memphis route\n",
      "downloading pictures in the  Memphis to Dallas route\n"
     ]
    }
   ],
   "source": [
    "# download photos along the way\n",
    "client = googlemaps.client.Client(key = api_key)\n",
    "\n",
    "# loop through each start city\n",
    "for index in range(len(cities)-1): \n",
    "    # get the current origin and destination\n",
    "    current_start = cities[index]\n",
    "    current_end = cities[index+1]\n",
    "    print('downloading pictures in the ', current_start, 'to', current_end, 'route')\n",
    "    # generate the current route\n",
    "    route = googlemaps.directions.directions(client = client,\n",
    "                                             origin = current_start,\n",
    "                                             destination = current_end, \n",
    "                                             mode = 'driving')\n",
    "    \n",
    "    # loop through each step\n",
    "    for step in route[0]['legs'][0]['steps'][0:-1]: \n",
    "        end_lat = step['end_location']['lat']\n",
    "        end_lng = step['end_location']['lng']\n",
    "        # make sure were out of the stopover cities, so if coordinates are outside of bounding boxes\n",
    "        start_minlat = bounding_box[current_start]['min_lat']\n",
    "        start_maxlat = bounding_box[current_start]['max_lat']\n",
    "        start_minlng = bounding_box[current_start]['min_lng']\n",
    "        start_maxlng = bounding_box[current_start]['max_lng']\n",
    "        \n",
    "        end_minlat = bounding_box[current_end]['min_lat']\n",
    "        end_maxlat = bounding_box[current_end]['max_lat']\n",
    "        end_minlng = bounding_box[current_end]['min_lng']\n",
    "        end_maxlng = bounding_box[current_end]['max_lng']\n",
    "        \n",
    "        # if we are outside of the cities, then look for images\n",
    "        if ((start_minlat < end_lat < start_maxlat) & (start_minlng < end_lng < start_maxlng) == False):\n",
    "            if ((end_minlat < end_lat < end_maxlat) & (end_minlng < end_lng < end_maxlng)== False):\n",
    "                urls = []\n",
    "                page_number = 1\n",
    "                \n",
    "                # looking for images within a 10 km radius, 300 for each end of step\n",
    "                while ((len(urls) < 300) and (page_number < 50)):\n",
    "                    photo_search = flickr.Photo.search(api_key = '4ea697590240af7b4ced24c4e46b3d41', \n",
    "                                                       lat = end_lat, \n",
    "                                                       lon = end_lng, \n",
    "                                                       per_page = 250,\n",
    "                                                       radius = 10,\n",
    "                                                       page = page_number,\n",
    "                                                       min_taken_date = 2016,\n",
    "                                                       content_type = 1,\n",
    "                                                       media = 'photos', \n",
    "                                                       extras = ['views']) \n",
    "                    for photo in photo_search:\n",
    "                        \n",
    "                        # only add pictures if they have more than 30 views\n",
    "                        if (photo.views > 30):\n",
    "                            try:\n",
    "                                url = photo.get('url_c')\n",
    "                                urls.append(url)\n",
    "                                path = '../photos_to_classify/' + str(file_number) + '.jpg'\n",
    "                                urllib.request.urlretrieve(url, path)\n",
    "                                image = Image.open(path)\n",
    "                                image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "                                image.save(path)\n",
    "                                current_route = current_start + ' ' + current_end\n",
    "                                # save relevant information about the images\n",
    "                                photo_info_df.loc[file_number-1] = [path, current_route, url,\n",
    "                                                                    photo.getLocation().latitude, \n",
    "                                                                    photo.getLocation().longitude]\n",
    "                                file_number += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                    page_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_info_df.to_csv('../datasets/all_pics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
